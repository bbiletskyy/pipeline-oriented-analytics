{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_oriented_analytics.dataframe import CsvDataFrame, ParquetDataFrame\n",
    "from pipeline_oriented_analytics import Phase\n",
    "\n",
    "features_df = ParquetDataFrame(f'../data/processed/{Phase.train.name.lower()}/features', spark)\n",
    "test_data_frac = 0.1\n",
    "test_features_df, train_features_df = features_df.randomSplit([test_data_frac, 1-test_data_frac])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "label_col = 'duration_min'\n",
    "assembler_estimator = Pipeline(stages=[\n",
    "    StringIndexer(inputCol='pickup_cell_6', handleInvalid='keep', outputCol='pickup_cell_6_idx'),\n",
    "    StringIndexer(inputCol='dropoff_cell_6', handleInvalid='keep', outputCol='dropoff_cell_6_idx'),\n",
    "    VectorAssembler(inputCols=['pickup_cell_6_idx', 'dropoff_cell_6_idx', 'distance', 'month', 'day_of_month', \n",
    "                               'day_of_week', 'hour', 'requests_pickup_cell', 'requests_dropoff_cell'], outputCol=\"features\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "dt_estimator = DecisionTreeRegressor(maxDepth=5, featuresCol='features', labelCol=label_col)\n",
    "rf_estimator = RandomForestRegressor(maxDepth=5, numTrees=40, featuresCol='features', labelCol=label_col)\n",
    "\n",
    "pipeline = Pipeline(stages=[])\n",
    "dt_stages = [assembler_estimator, dt_estimator]\n",
    "rf_stages = [assembler_estimator, rf_estimator]\n",
    "dt_pipeline = Pipeline(stages=dt_stages)\n",
    "rf_pipeline = Pipeline(stages=rf_stages)\n",
    "\n",
    "dt_grid = ParamGridBuilder().baseOn({pipeline.stages: dt_stages})\\\n",
    "                            .addGrid(dt_estimator.maxDepth, [2, 5, 7, 9])\\\n",
    "                            .build()\n",
    "\n",
    "rf_grid = ParamGridBuilder().baseOn({pipeline.stages: rf_stages})\\\n",
    "                            .addGrid(rf_estimator.maxDepth, [5, 7])\\\n",
    "                            .addGrid(rf_estimator.numTrees, [10, 20])\\\n",
    "                            .build()\n",
    "grid = dt_grid + rf_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing mae evaluator and 3-fold cross-validator...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval_metric = 'mae'\n",
    "folds = 3\n",
    "print(f'Preparing {eval_metric} evaluator and {folds}-fold cross-validator...')\n",
    "mae_evaluator = RegressionEvaluator(metricName=eval_metric, labelCol=label_col)\n",
    "cross_val = CrossValidator(estimatorParamMaps=grid, estimator=pipeline,\n",
    "                           evaluator=mae_evaluator, numFolds=folds, parallelism=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Training complete, duration: 0:04:06.047454\n",
      "Best model: [PipelineModel_ae97e0395eea, DecisionTreeRegressionModel (uid=DecisionTreeRegressor_60580be5e8f0) of depth 7 with 243 nodes]\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "print(f'Training...')\n",
    "start = timer()\n",
    "cross_val_model = cross_val.fit(train_features_df)\n",
    "end = timer()\n",
    "print(f'Training complete, duration: {timedelta(seconds=end-start)}')\n",
    "print(f'Best model: {cross_val_model.bestModel.stages}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../model/trip_duration_min\n",
      "Model saved...\n"
     ]
    }
   ],
   "source": [
    "model_path = '../model/trip_duration_min'\n",
    "print(f'Saving model to {model_path}')\n",
    "cross_val_model.bestModel.write().overwrite().save(model_path)\n",
    "print(f'Model saved...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model MAE: 6.511276324357712\n"
     ]
    }
   ],
   "source": [
    "predictions_df = cross_val_model.transform(test_features_df)\n",
    "mae_cv = RegressionEvaluator(labelCol=label_col, metricName=eval_metric).evaluate(predictions_df)\n",
    "print(f'Best model MAE: {mae_cv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| avg(duration_min)|\n",
      "+------------------+\n",
      "|15.617653213342992|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df.groupby().agg(f.avg(label_col)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pipeline-oriented-analytics]",
   "language": "python",
   "name": "conda-env-pipeline-oriented-analytics-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
